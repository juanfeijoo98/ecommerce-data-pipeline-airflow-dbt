# ğŸ›’ E-commerce Data Pipeline (Airflow + dbt + Docker + Postgres)

This project demonstrates an **end-to-end data engineering pipeline** for an e-commerce sales dataset.  
It follows the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** and integrates **Airflow, dbt, Docker, and Postgres**.

---

## ğŸš€ Features
- **Airflow** orchestrates the entire workflow:
  - `batch_sales` â†’ moves raw CSVs to *bronze*, validates, converts to parquet (*silver*), loads into Postgres, then triggers dbt.
  - `weather_ingest_v2` â†’ ingests external weather data.
  - `dbt_run` â†’ executes dbt transformations and tests.
- **dbt** models implement:
  - Staging models for raw sources
  - Dimension and fact tables
  - Data quality tests (`not_null`, `unique`, `relationships`)
- **Docker Compose** runs all services in isolated containers (Airflow, Postgres, dbt).
- **Medallion Architecture** applied:
  - **Bronze**: raw ingested data
  - **Silver**: cleaned & standardized parquet files
  - **Gold**: analytics-ready tables (facts, dims, marts)

---

## ğŸ—ï¸ Architecture
```mermaid
flowchart LR
    A[CSV Sales Data] -->|Ingest| B(Bronze Layer)
    B -->|Validation & Parquet| C(Silver Layer)
    C -->|Load to Postgres| D(dbt Staging Models)
    D --> E[Dims & Facts]
    E --> F[Gold Layer - Analytics]


.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ batch_sales.py
â”‚   â”‚   â”œâ”€â”€ weather_ingest_v2.py
â”‚   â”‚   â””â”€â”€ dbt_run.py
â”‚   â””â”€â”€ include/dbt_profiles/profiles.yml
â”œâ”€â”€ de_dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ dims/
â”‚   â”‚   â”œâ”€â”€ facts/
â”‚   â”‚   â””â”€â”€ gold/
â”‚   â””â”€â”€ schema.yml
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ data_seed/   # raw CSV files
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ How to Run
git clone https://github.com/juanfeijoo98/ecommerce-data-pipeline-airflow-dbt.git
cd ecommerce-data-pipeline-airflow-dbt

Start services with Docker

docker compose up -d
Access Airflow

Web UI â†’ http://localhost:8080

User: admin | Password: admin

Run DAGs

Trigger batch_sales (ingests and loads sales data)

Trigger weather_ingest_v2 (ingests external data)

dbt models are executed automatically via dbt_run

ğŸ“Š Example Outputs

Airflow DAG Graph View
(screenshot here)

dbt test results
(screenshot here)

Final schema (Gold Layer): fact_orders, dim_customers, dim_products, enriched with weather data.

ğŸ› ï¸ Tech Stack

Airflow â€“ orchestration

dbt (Postgres) â€“ transformations & tests

Docker Compose â€“ containerized environment

Postgres â€“ data warehouse

Python (pandas, SQLAlchemy) â€“ data ingestion

ğŸ¯ Use Cases

Retail sales analytics

ETL/ELT best practices

Medallion architecture example

Portfolio project for Data Engineering interviews

ğŸ“Œ Author

ğŸ‘¤ Juan Pablo Feijoo
Aspiring Data Engineer | Data Scientist | Passionate about building scalable data pipelines

